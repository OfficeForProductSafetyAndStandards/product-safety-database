name: Publish Staging redacted export to S3

on:
  pull_request:
  # TODO: Set this to manual
  # workflow_dispatch:

jobs:
  create_redacted_schema_sql:
    name: Create schema artifact
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v1

      - name: Set up Ruby
        uses: ruby/setup-ruby@v1

      - name: Set up Node
        uses: actions/setup-node@v2
        with:
          node-version: 14.17.x

      - name: Install PostgreSQL client
        run: sudo apt-get install libpq-dev

      - uses: actions/cache@v1
        with:
          path: vendor/bundle
          key: ${{ runner.os }}-gems-no-dev-${{ hashFiles('**/Gemfile.lock') }}
          restore-keys: |
            ${{ runner.os }}-gems-

      - name: Install RubyGems
        run: |
          gem install bundler --no-doc
          bundle config path vendor/bundle
          bundle check || bundle install --without development --jobs=4 --retry=3

      - name: Install yarn
        run: npm install yarn -g

      - name: Get yarn cache
        id: yarn-cache
        run: echo "::set-output name=dir::$(yarn cache dir)"

      - uses: actions/cache@v1
        with:
          path: ${{ steps.yarn-cache.outputs.dir }}
          key: ${{ runner.os }}-yarn-${{ hashFiles('**/yarn.lock') }}
          restore-keys: |
            ${{ runner.os }}-yarn-

      - name: Install Node.js dependencies
        run: yarn install

      - name: Run redacted export rake task
        env:
          # NOTE: These services do not need to be running for this job
          DATABASE_URL: postgres://postgres@localhost:5432
          ELASTICSEARCH_URL: http://localhost:9200
          REDIS_URL: redis://localhost:6379
          RAILS_ENV: test
          ANTIVIRUS_URL: http://localhost:3006/safe
          ANTIVIRUS_USERNAME: av
          ANTIVIRUS_PASSWORD: password
          PSD_HOST: example.com
          EMAIL_WHITELIST_ENABLED: false
          TWO_FACTOR_AUTHENTICATION_ENABLED: false
          CI: true
        run: |
          bin/rails redacted_export:generate_sql > tmp/create_redacted_schema.sql

      - uses: actions/upload-artifact@v2
        with:
          name: create_redacted_schema.sql
          path: tmp/create_redacted_schema.sql
          retention-days: 7

  export_database:
    name: Export and upload redacted database dump
    runs-on: ubuntu-latest
    needs: create_redacted_schema_sql

    steps:
      - name: Install cf client
        env:
          CF_CLI_VERSION: v7
        run: |
          mkdir -p $GITHUB_WORKSPACE/bin
          curl -L "https://packages.cloudfoundry.org/stable?release=linux64-binary&version=${CF_CLI_VERSION}" | tar -zx -C $GITHUB_WORKSPACE/bin
          echo "$GITHUB_WORKSPACE/bin" >> $GITHUB_PATH

      - name: Install conduit plugin
        run: cf install-plugin conduit -f

      - name: Install PostgreSQL client
        run: sudo apt-get install libpq-dev

      - uses: actions/download-artifact@v2
        with:
          name: create_redacted_schema.sql

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.DB_UPLOAD_STAGING_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.DB_UPLOAD_STAGING_AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-2

      - name: Export database
        env:
          SPACE: staging
          CF_USERNAME: ${{ secrets.PaaSUsernameStaging }}
          CF_PASSWORD: ${{ secrets.PaaSPasswordStaging }}
          S3_BUCKET: psd-staging-database-dumps
        run: |
          TIMESTAMP=$(date +'%F-%H-%M-%S')
          TEMP_DB_SERVICE_NAME=psd-redacted-export-$TIMESTAMP
          FINAL_S3_URL="s3://${S3_BUCKET}/${TIMESTAMP}.sql.gz"
          echo "::set-output name=temp-db-service-name::$TEMP_DB_SERVICE_NAME"
          echo "::set-output name=final-s3-url::$FINAL_S3_URL"
          exit 1
          cf api api.london.cloud.service.gov.uk
          cf auth
          cf target -o 'beis-opss' -s $SPACE
          PSD_DATABASE_GUID=$(cf service psd-database --guid)
          PSD_DATABASE_PLAN=$(cf service psd-database | awk -F ':' '$1=="plan" { gsub(/ /,"", $NF); print $NF }')
          cf create-service postgres $PSD_DATABASE_PLAN $TEMP_DB_SERVICE_NAME -c "{\"restore_from_latest_snapshot_of\": \"$PSD_DATABASE_GUID\"}"
          echo "::group::Waiting for db"
          until cf service $TEMP_DB_SERVICE_NAME > /tmp/db_exists && grep -E "create succeeded|update succeeded" /tmp/db_exists; do sleep 30; echo "Waiting for db..."; done
          echo "::endgroup::"
          echo "::group::Schema dump"
          cf conduit $TEMP_DB_SERVICE_NAME -- psql < create_redacted_schema.sql
          cf conduit $TEMP_DB_SERVICE_NAME -- pg_dump --schema=redacted --file=psd_redacted_export.sql --no-acl --no-owner --quote-all-identifiers --format=p --inserts --encoding=UTF8
          echo "::endgroup::"
          echo Waiting for conduit app to terminate...
          sleep 30
          cf delete-service $TEMP_DB_SERVICE_NAME -f
          cf logout
          gzip --keep psd_redacted_export.sql
          aws s3 cp psd_redacted_export.sql.gz $FINAL_S3_URL

      - name: Alert team via Slack of export failure
        if: failure()
        env:
          SLACK_WEBHOOK: ${{ secrets.SlackWebhookUrl }}
        run: |
          curl -X POST --data-urlencode "payload={\"channel\": \"#alerts\", \"username\": \"exportbot\", \"text\": \"Redacted export has failed!\n\nCF Service: ${{ steps.export-database.outputs.temp-db-service-name }}\nS3 destination: ${{ steps.export-database.outputs.final-s3-url }}\", \"icon_emoji\": \":disappointed:\"}" $SLACK_WEBHOOK

      - name: Try to delete the temporary DB
        if: failure()
        env:
          SPACE: staging
          CF_USERNAME: ${{ secrets.PaaSUsernameStaging }}
          CF_PASSWORD: ${{ secrets.PaaSPasswordStaging }}
        run: |
          cf api api.london.cloud.service.gov.uk
          cf auth
          cf target -o 'beis-opss' -s $SPACE
          cf delete-service ${{ steps.export-database.outputs.temp-db-service-name }} -f
          cf logout
